{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f8f287d-9ee8-46f0-b486-b22dd9e1ba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43cfcc8-b034-4f47-876c-64c620f317ad",
   "metadata": {},
   "source": [
    "# Stock scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8135d35-3cff-49b1-b75f-a3275ec85f35",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e56564a-295a-484b-add8-a9cbcc81e942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sp500_list():\n",
    "    \"\"\"\n",
    "    Get a list of S&P 500 company symbols by reading the Wikipedia page.\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    tables = pd.read_html(url)\n",
    "    sp500_table = tables[0]\n",
    "    sp500_symbols = sp500_table[\"Symbol\"].tolist()\n",
    "    return sp500_symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac8e2a7-9f43-4c43-b277-377cfd86b807",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "610e97a9-9dfa-4acc-8e31-d15019a00c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY_ALPHAVANTAGE = '76I7ZXLX7S7BSRAX'\n",
    "API_KEY_FAM = 'NdLNkc2mzBnSVEvOgyOqB3CBGN4YBm4v'\n",
    "\n",
    "BASE_URL_FAM = 'https://financialmodelingprep.com/api/v3'\n",
    "\n",
    "REQUEST_LIMIT = 250\n",
    "req_count = 0\n",
    "YEARS = 15\n",
    "\n",
    "FIN_DATA_DIR = '../financial_data/'\n",
    "DATA_DIR = '../data/'\n",
    "\n",
    "# List of required metrics\n",
    "required_metrics = [\n",
    "    \"GrossProfit\",\n",
    "    \"Revenues\",\n",
    "    \"NetIncomeLoss\",\n",
    "    \"StockholdersEquity\",\n",
    "    \"Liabilities\",\n",
    "    \"AssetsNoncurrent\",\n",
    "    \"NetCashProvidedByUsedInOperatingActivities\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f97ffc-4dcc-4b1f-a147-4c762f7c1cec",
   "metadata": {},
   "source": [
    "## Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f16e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list of available tickers using the free FMP subsribtion\n",
    "FAM_free_access = [\n",
    "    'AAPL', 'TSLA', 'AMZN', 'MSFT', 'NVDA', 'GOOGL', 'META', 'NFLX', 'JPM', 'V', 'BAC', 'AMD', 'PYPL', 'DIS', 'T', 'PFE', 'COST', 'INTC', 'KO', 'TGT', 'NKE', 'SPY', 'BA', 'BABA', 'XOM', 'WMT', 'GE', 'CSCO', 'VZ', 'JNJ', 'CVX', 'PLTR', 'SQ', 'SHOP', 'SBUX', 'SOFI', 'HOOD', 'RBLX', 'SNAP', 'UBER', 'FDX', 'ABBV', 'ETSY', 'MRNA', 'LMT', 'GM', 'F', 'RIVN', 'LCID', 'CCL', 'DAL', 'UAL', 'AAL', 'TSM', 'SONY', 'ET', 'NOK', 'MRO', 'COIN', 'SIRI', 'RIOT', 'CPRX', 'VWO', 'SPYG', 'ROKU', 'VIAC', 'ATVI', 'BIDU', 'DOCU', 'ZM', 'PINS', 'TLRY', 'WBA', 'MGM', 'NIO', 'C', 'GS', 'WFC', 'ADBE', 'PEP', 'UNH', 'CARR', 'FUBO', 'HCA', 'TWTR', 'BILI', 'RKT'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff5bd9a2-8121-40b7-88f7-a9a47e5294f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sp500_tickers.csv already exists, therefore import it.\n",
      "data_progress_sp500.csv already exists, therefore import it.\n",
      "data_progress_free.csv already exists, therefore import it.\n"
     ]
    }
   ],
   "source": [
    "# Create .csv files to keep track of the company data that I have and still need to get\n",
    "if not os.path.exists(f\"{DATA_DIR}sp500_tickers.csv\"):\n",
    "    sp500_list = get_sp500_list()\n",
    "    df_sp500 = pd.DataFrame(sp500_list)\n",
    "    df_sp500.to_csv(f\"{DATA_DIR}sp500_tickers.csv\", index=False)\n",
    "else:\n",
    "    print(\"sp500_tickers.csv already exists, therefore import it.\")\n",
    "    df_sp500 = pd.read_csv(f\"{DATA_DIR}sp500_tickers.csv\")\n",
    "\n",
    "if not os.path.exists(f\"{DATA_DIR}data_progress_sp500.csv\"):\n",
    "    df_progress = pd.DataFrame(columns=['ticker', 'income_done', 'balance_done', 'cashflow_done'])\n",
    "    df_progress['ticker'] = sp500_list\n",
    "    df_progress['income_done'] = False\n",
    "    df_progress['balance_done'] = False\n",
    "    df_progress['cashflow_done'] = False\n",
    "    df_progress.set_index('ticker', inplace=True)\n",
    "    df_progress.to_csv(f\"{DATA_DIR}data_progress_sp500.csv\")\n",
    "    df_data_progress = df_progress\n",
    "else: \n",
    "    print(\"data_progress_sp500.csv already exists, therefore import it.\")\n",
    "    df_data_progress_sp500 = pd.read_csv(f\"{DATA_DIR}data_progress_sp500.csv\", index_col='ticker')\n",
    "\n",
    "if not os.path.exists(f\"{DATA_DIR}data_progress_free.csv\"):\n",
    "    df_progress = pd.DataFrame(columns=['ticker', 'income_done', 'balance_done', 'cashflow_done'])\n",
    "    df_progress['ticker'] = FAM_free_access\n",
    "    df_progress['income_done'] = False\n",
    "    df_progress['balance_done'] = False\n",
    "    df_progress['cashflow_done'] = False\n",
    "    df_progress.set_index('ticker', inplace=True)\n",
    "    df_progress.to_csv(f\"{DATA_DIR}data_progress_free.csv\")\n",
    "    df_data_progress_free = df_progress\n",
    "else: \n",
    "    print(\"data_progress_free.csv already exists, therefore import it.\")\n",
    "    df_data_progress_free = pd.read_csv(f\"{DATA_DIR}data_progress_free.csv\", index_col='ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "844eb560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       income_done balance_done cashflow_done\n",
      "ticker                                       \n",
      "AAPL          True         True          True\n",
      "TSLA          True         True          True\n",
      "AMZN          True         True          True\n",
      "MSFT          True         True          True\n",
      "NVDA          True         True          True\n",
      "...            ...          ...           ...\n",
      "FUBO          True         True          True\n",
      "HCA           True         True          True\n",
      "TWTR          True         True          True\n",
      "BILI          True         True          True\n",
      "RKT           True         True          True\n",
      "\n",
      "[87 rows x 3 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 87 entries, AAPL to RKT\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   income_done    87 non-null     object\n",
      " 1   balance_done   87 non-null     object\n",
      " 2   cashflow_done  87 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_data_progress_free)\n",
    "print(df_data_progress_free.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b9c18b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPY is not in the s&p500 list\n",
      "BABA is not in the s&p500 list\n",
      "SQ is not in the s&p500 list\n",
      "SHOP is not in the s&p500 list\n",
      "SOFI is not in the s&p500 list\n",
      "HOOD is not in the s&p500 list\n",
      "RBLX is not in the s&p500 list\n",
      "SNAP is not in the s&p500 list\n",
      "ETSY is not in the s&p500 list\n",
      "RIVN is not in the s&p500 list\n",
      "LCID is not in the s&p500 list\n",
      "AAL is not in the s&p500 list\n",
      "TSM is not in the s&p500 list\n",
      "SONY is not in the s&p500 list\n",
      "ET is not in the s&p500 list\n",
      "NOK is not in the s&p500 list\n",
      "MRO is not in the s&p500 list\n",
      "SIRI is not in the s&p500 list\n",
      "RIOT is not in the s&p500 list\n",
      "CPRX is not in the s&p500 list\n",
      "VWO is not in the s&p500 list\n",
      "SPYG is not in the s&p500 list\n",
      "ROKU is not in the s&p500 list\n",
      "VIAC is not in the s&p500 list\n",
      "ATVI is not in the s&p500 list\n",
      "BIDU is not in the s&p500 list\n",
      "DOCU is not in the s&p500 list\n",
      "ZM is not in the s&p500 list\n",
      "PINS is not in the s&p500 list\n",
      "TLRY is not in the s&p500 list\n",
      "NIO is not in the s&p500 list\n",
      "FUBO is not in the s&p500 list\n",
      "TWTR is not in the s&p500 list\n",
      "BILI is not in the s&p500 list\n",
      "RKT is not in the s&p500 list\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list_tickers = df_data_progress_sp500.index.tolist()\n",
    "\n",
    "# check if the available tickers is in the S&P500 list\n",
    "for ticker in FAM_free_access:\n",
    "    if ticker not in list_tickers:\n",
    "        print(f\"{ticker} is not in the s&p500 list\")\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "611c8d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_income_cols = ['reportedCurrency', 'cik', 'fillingDate', 'acceptedDate', 'calendarYear', 'link', 'finalLink', 'eps', 'epsdiluted', 'weightedAverageShsOut', 'weightedAverageShsOutDil', 'grossProfitRatio', 'ebitdaratio', 'operatingIncomeRatio', 'incomeBeforeTaxRatio']\n",
    "\n",
    "drop_balance_columns = ['reportedCurrency', 'cik', 'fillingDate', 'acceptedDate', 'link', 'calendarYear', 'finalLink']\n",
    "\n",
    "drop_cashflow_columns = ['reportedCurrency', 'cik', 'fillingDate', 'acceptedDate', 'link', 'calendarYear', 'finalLink']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31582696",
   "metadata": {},
   "source": [
    "## Get the statements\n",
    "\n",
    "Retreive the Income, Balance and Cashflow statements for all the companies\n",
    "For the time being only get the statements of the companies that I have free access to, but will late change this to all of the companies on the S&P500. \n",
    "\n",
    "Also try to combine the statements when I get them inputted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2210a991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "income_done      object\n",
       "balance_done     object\n",
       "cashflow_done    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_progress_free.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb5b1f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tickers = df_data_progress_free.index.tolist()\n",
    "\n",
    "# Loop through the tickers and get their income, balance and cashflow statements\n",
    "for ticker in list_tickers:\n",
    "    \n",
    "    # Ensure the correct amount of API calls\n",
    "    if not ((REQUEST_LIMIT - req_count) > 3):\n",
    "        print(f\"Request limit reached, curr count: {req_count}\")\n",
    "        break\n",
    "\n",
    "    row = df_data_progress_free.loc[ticker]\n",
    "\n",
    "    ### INCOME STATEMENT ###\n",
    "    if (row['income_done'] == False):\n",
    "        print(f\"Getting income statement for {ticker}...\")\n",
    "        url = f\"{BASE_URL_FAM}/income-statement/{ticker}?period=annual&limit={YEARS}&apikey={API_KEY_FAM}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            req_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Request failed for {ticker}: {e}\")\n",
    "            break\n",
    "\n",
    "        print(f\"Status Code: {response.status_code}\")\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error fetching {ticker}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        if response.ok:\n",
    "            data = response.json()\n",
    "            if data:\n",
    "                df = pd.DataFrame(data)\n",
    "                df = df.drop(columns=drop_income_cols)\n",
    "                df.to_csv(f\"{FIN_DATA_DIR}{ticker}_income.csv\", index=False)\n",
    "                df_data_progress_free.loc[ticker, 'income_done'] = True\n",
    "                print(f\"Income downloaded for {ticker}\")\n",
    "            else:\n",
    "                print(f\"No data returned for {ticker}\")\n",
    "                df_data_progress_free.loc[ticker, 'income_done'] = 'no_data'\n",
    "        else:\n",
    "            print(f\"Failed to get income for {ticker}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    ### BALANCE STATEMENT ###\n",
    "    if (row['balance_done'] == False):\n",
    "        print(f\"Getting balance statement for {ticker}...\")\n",
    "        url = f\"{BASE_URL_FAM}/balance-sheet-statement/{ticker}?limit={YEARS}&apikey={API_KEY_FAM}\"\n",
    "        \n",
    "        # Ensure the request through the api works\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            req_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Request failed for {ticker}: {e}\")\n",
    "            break\n",
    "\n",
    "        print(f\"Status Code: {response.status_code}\")\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error fetching balance sheet for {ticker}: {response.text}\")\n",
    "        elif response.ok:\n",
    "            data = response.json()\n",
    "            if data:\n",
    "                df = pd.DataFrame(data).drop(columns=drop_balance_columns)\n",
    "                df.to_csv(f\"{FIN_DATA_DIR}{ticker}_balance.csv\", index=False)\n",
    "                df_data_progress_free.loc[ticker, 'balance_done'] = True\n",
    "                print(f\"Balance sheet downloaded for {ticker}\")\n",
    "            else:\n",
    "                print(f\"No data returned for {ticker}\")\n",
    "                df_data_progress_free.loc[ticker, 'balance_done'] = 'no_data'\n",
    "        time.sleep(1)\n",
    "\n",
    "    ### CASHFLOW STATEMENT ###\n",
    "    if (row['cashflow_done'] == False):\n",
    "        print(f\"Getting Cash flow statement for {ticker}...\")\n",
    "        url = f\"{BASE_URL_FAM}/cash-flow-statement/{ticker}?limit={YEARS}&apikey={API_KEY_FAM}\"\n",
    "\n",
    "        # Ensure correctness of API call\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            req_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Request failed for {ticker}: {e}\")\n",
    "            break\n",
    "\n",
    "        print(f\"Status Code: {response.status_code}\")\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error fetching cash flow for {ticker}: {response.text}\")\n",
    "        elif response.ok:\n",
    "            data = response.json()\n",
    "            if data:\n",
    "                df = pd.DataFrame(data).drop(columns=drop_cashflow_columns)\n",
    "                df.to_csv(f\"{FIN_DATA_DIR}{ticker}_cashflow.csv\", index=False)\n",
    "                df_data_progress_free.loc[ticker, 'cashflow_done'] = True\n",
    "                print(f\"Cash flow downloaded for {ticker}\")\n",
    "            else:\n",
    "                print(f\"No data returned for {ticker}\")\n",
    "                df_data_progress_free.loc[ticker, 'cashflow_done'] = 'no_data'\n",
    "        time.sleep(1)\n",
    "        print()\n",
    "\n",
    "        \n",
    "df_data_progress_free.to_csv(f\"{DATA_DIR}data_progress_free.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f28869",
   "metadata": {},
   "source": [
    "## Combine all statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4c7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining statements of AAPL\n",
      "Combining statements of TSLA\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Combine all of the data that was collected\n",
    "combined_data = []\n",
    "list_tickers = df_data_progress_free.index.tolist()\n",
    "count = 0\n",
    "\n",
    "if not os.path.exists(f\"{DATA_DIR}combined_data.csv\"):\n",
    "    df_progress = pd.read_csv(f\"{DATA_DIR}data_progress_free.csv\", index_col='ticker')\n",
    "    for ticker in list_tickers:\n",
    "\n",
    "        # limit for testing\n",
    "        # if (count > 1): break\n",
    "            \n",
    "        try:\n",
    "            print(f\"Combining statements of {ticker}\")\n",
    "            income_df = pd.read_csv(f\"{FIN_DATA_DIR}{ticker}_income.csv\")\n",
    "            balance_df = pd.read_csv(f\"{FIN_DATA_DIR}{ticker}_balance.csv\")\n",
    "            cashflow_df = pd.read_csv(f\"{FIN_DATA_DIR}{ticker}_cashflow.csv\")\n",
    "\n",
    "            # Extract year column in all dataframes\n",
    "            income_df['year'] = pd.to_datetime(income_df['date']).dt.year\n",
    "            balance_df['year'] = pd.to_datetime(balance_df['date']).dt.year\n",
    "            cashflow_df['year'] = pd.to_datetime(cashflow_df['date']).dt.year\n",
    "\n",
    "            # Merge all three on 'year'\n",
    "            df_merged = income_df.merge(balance_df, on='year', suffixes=('', '_bal'))\n",
    "            df_merged = df_merged.merge(cashflow_df, on='year', suffixes=('', '_cf'))\n",
    "\n",
    "            df_merged['ticker'] = ticker\n",
    "            df_merged.drop(columns=['symbol', 'date_bal', 'symbol_bal', 'period_bal', 'date_cf', 'symbol_cf', 'period_cf'], inplace=True)\n",
    "\n",
    "            # Reorder the dataframe\n",
    "            cols = df_merged.columns.tolist()\n",
    "            cols.insert(0, cols.pop(cols.index('year')))\n",
    "            cols.insert(1, cols.pop(cols.index('ticker')))\n",
    "\n",
    "            df_merged = df_merged[cols]\n",
    "            combined_data.append(df_merged)\n",
    "\n",
    "            # count +=1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {ticker} due to error: {e}\")\n",
    "\n",
    "\n",
    "    df_full = pd.concat(combined_data, ignore_index=True)\n",
    "    df_full.to_csv(f\"{DATA_DIR}combined_data.csv\", index=False)\n",
    "else:\n",
    "    print(\"combined_data.csv already exists\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dac19d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
